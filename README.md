# Gesture Recognition Project

A comprehensive gesture recognition system that implements multiple approaches for detecting and classifying Rock-Paper-Scissors hand gestures using computer vision and machine learning.

## ğŸ¯ Features

- **Multiple Recognition Methods**: 
  - YOLO Object Detection
  - YOLO Classification
  - MediaPipe Hand Landmarks
- **Real-time Webcam Detection**: Live gesture recognition with webcam input
- **Training Scripts**: Complete training pipelines for custom models
- **Pre-trained Models**: Ready-to-use models for immediate testing
- **Cross-platform Support**: Works on Windows, macOS, and Linux

## ğŸ“ Project Structure

```
GestureRecognition/
â”œâ”€â”€ Classification/              # YOLO Classification approach
â”‚   â”œâ”€â”€ train_classification_rps.py
â”‚   â”œâ”€â”€ rps_classification_webcam.py
â”‚   â”œâ”€â”€ test_classification_model.py
â”‚   â”œâ”€â”€ dataset.yaml
â”‚   â””â”€â”€ rock_paper_scissors_robust/
â”‚       â””â”€â”€ weights/
â”‚           â”œâ”€â”€ best.pt
â”‚           â”œâ”€â”€ last.pt
â”‚           â””â”€â”€ epoch0.pt
â”œâ”€â”€ ObjectRecognition/           # YOLO Object Detection approach
â”‚   â”œâ”€â”€ train_detection_rps.py
â”‚   â”œâ”€â”€ rps_detection_webcam.py
â”‚   â”œâ”€â”€ downloadDataset.py
â”‚   â”œâ”€â”€ rps_yolo_dataset.yaml
â”‚   â””â”€â”€ rps_yolo_training/
â”‚       â””â”€â”€ rock_paper_scissors_detection/
â”‚           â””â”€â”€ weights/
â”‚               â”œâ”€â”€ best.pt
â”‚               â””â”€â”€ last.pt
â”œâ”€â”€ MediapipesGestureRecognition/ # MediaPipe Hand Landmarks approach
â”‚   â”œâ”€â”€ rps_mediapipe_landmarks.py
â”‚   â””â”€â”€ hand_landmarker.task
â””â”€â”€ archive (1)/                 # Dataset files
    â”œâ”€â”€ RPS_Raw_Images/
    â”œâ”€â”€ RPS_YOLO_Annotated/
    â””â”€â”€ Rock-Paper-Scissors/
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- Webcam or camera device
- CUDA-compatible GPU (optional, for faster training)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/gesture-recognition.git
   cd gesture-recognition
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download datasets (optional):**
   ```bash
   cd ObjectRecognition
   python downloadDataset.py
   ```

### Usage

#### 1. YOLO Classification (Recommended for beginners)

```bash
cd Classification
python rps_classification_webcam.py
```

#### 2. YOLO Object Detection

```bash
cd ObjectRecognition
python rps_detection_webcam.py
```

#### 3. MediaPipe Hand Landmarks

```bash
cd MediapipesGestureRecognition
python rps_mediapipe_landmarks.py
```

## ğŸ® How to Use

1. **Run any of the webcam scripts**
2. **Position your hand in front of the camera**
3. **Make Rock, Paper, or Scissors gestures**
4. **The system will detect and classify your gesture in real-time**
5. **Press 'q' to quit**

## ğŸ‹ï¸ Training Your Own Models

### Classification Model Training

```bash
cd Classification
python train_classification_rps.py
```

### Object Detection Model Training

```bash
cd ObjectRecognition
python train_detection_rps.py
```

## ğŸ“Š Model Performance

| Method | Accuracy | Speed | GPU Required |
|--------|----------|-------|--------------|
| YOLO Classification | ~95% | Fast | Optional |
| YOLO Detection | ~90% | Medium | Recommended |
| MediaPipe Landmarks | ~85% | Very Fast | No |

## ğŸ› ï¸ Technical Details

### YOLO Classification
- **Model**: YOLOv11n-cls
- **Input Size**: 224x224
- **Classes**: Rock, Paper, Scissors
- **Training**: 10 epochs with data augmentation

### YOLO Object Detection
- **Model**: YOLOv11n
- **Input Size**: 640x640
- **Classes**: Rock, Paper, Scissors
- **Training**: 30 epochs with extensive augmentation

### MediaPipe Hand Landmarks
- **Framework**: MediaPipe Tasks API
- **Landmarks**: 21 hand keypoints
- **Method**: Vector-based gesture recognition
- **Real-time**: 30+ FPS

## ğŸ“ˆ Data Augmentation

The training scripts include comprehensive data augmentation:

- **HSV variations**: Hue, saturation, and value adjustments
- **Geometric transforms**: Rotation, translation, scaling, shearing
- **Advanced techniques**: Mosaic, mixup, copy-paste augmentation
- **Auto augmentation**: RandAugment for optimal performance

## ğŸ”§ Configuration

### Dataset Configuration

Each approach uses YAML configuration files:

- `Classification/dataset.yaml` - Classification dataset paths
- `ObjectRecognition/rps_yolo_dataset.yaml` - Detection dataset paths

### Model Parameters

Key parameters can be adjusted in the training scripts:

- **Epochs**: Number of training iterations
- **Batch size**: Training batch size
- **Image size**: Input image dimensions
- **Learning rate**: Model learning rate
- **Data augmentation**: Augmentation parameters

## ğŸ› Troubleshooting

### Common Issues

1. **Webcam not detected:**
   - Ensure your camera is connected and not used by other applications
   - Try changing the camera index in `cv2.VideoCapture(0)` to `cv2.VideoCapture(1)`

2. **CUDA out of memory:**
   - Reduce batch size in training scripts
   - Use CPU training by setting `device='cpu'`

3. **Model files not found:**
   - Ensure you've trained the models first
   - Check that the weights are in the correct directories

4. **Poor detection accuracy:**
   - Ensure good lighting conditions
   - Keep your hand clearly visible in the camera frame
   - Try different gesture positions

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics) for the YOLO implementation
- [MediaPipe](https://mediapipe.dev/) for hand landmark detection
- [Kaggle](https://www.kaggle.com/) for the Rock-Paper-Scissors dataset
- The computer vision community for inspiration and resources

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [Issues](https://github.com/yourusername/gesture-recognition/issues) page
2. Create a new issue with detailed information
3. Include your system specifications and error messages

## ğŸ”® Future Enhancements

- [ ] Support for more gesture types
- [ ] Improved accuracy ML models

---


